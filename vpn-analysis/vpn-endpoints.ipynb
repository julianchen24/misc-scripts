{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VPN Analysis Notebook to extract and enhance VPN endpoint data for further analysis of the upstream networks that host and facilitate VPN providers.\n",
    "\n",
    "Wrote this because I wanted to empirically prove the anecdotes that the overwhelming majority of VPN providers are reliant on M247, which represents the single biggest point of failure for a hypothetical attack for a global passive adversary.\n",
    "\n",
    "Acknowledgements:\n",
    "* qdm12 (Quentin McGaw) <https://github.com/qdm12> for Gluetun\n",
    "* Frank Denis <iptoasn.com> for IPToASN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved: vpn_servers_all_providers.csv\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Using the GlueTun <https://github.com/qdm12/gluetun> project's\n",
    "server.json directory of VPN endpoints, extract and write to a CSV\n",
    "file for further processing.\n",
    "'''\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import requests\n",
    "\n",
    "# URL of Gluetun servers.json file\n",
    "json_url = 'https://raw.githubusercontent.com/qdm12/gluetun/refs/heads/master/internal/storage/servers.json'\n",
    "\n",
    "# Fetch the JSON server data from the Gluetun project\n",
    "response = requests.get(json_url)\n",
    "\n",
    "data = response.json()  # Parse the JSON data\n",
    "\n",
    "# Define the CSV file where the data will be saved\n",
    "csv_file_path = 'vpn_servers_all_providers.csv'\n",
    "\n",
    "# Define the headers for the CSV file\n",
    "headers = ['provider', 'vpn', 'country', 'region', 'city', 'server_name', 'hostname', 'wgpubkey', 'ips', 'tcp', 'udp']\n",
    "\n",
    "# Open the CSV file for writing\n",
    "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    # Create a CSV writer object\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=headers)\n",
    "    \n",
    "    # Write the headers to the CSV file\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Iterate over each provider in the JSON data\n",
    "    for provider_name, provider_data in data.items():\n",
    "        # Skip non-dictionary items\n",
    "        if not isinstance(provider_data, dict) or 'servers' not in provider_data:\n",
    "            continue\n",
    "        \n",
    "        # Iterate over the servers for each provider to write to CSV\n",
    "        for server in provider_data['servers']:\n",
    "            row = {header: server.get(header, '') for header in headers if header not in ['provider']}\n",
    "            row['provider'] = provider_name  # Add provider name to row\n",
    "            row['ips'] = ', '.join(server.get('ips', []))\n",
    "            row['tcp'] = str(server.get('tcp', ''))\n",
    "            row['udp'] = str(server.get('udp', ''))\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Output the created CSV\n",
    "print(f\"CSV file saved: {csv_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Download and extract the IP to ASN dataset.\n",
    "'''\n",
    "\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "url = 'https://iptoasn.com/data/ip2asn-combined.tsv.gz'\n",
    "response = requests.get(url, stream=True)\n",
    "gz_file_path = 'ip2asn-combined.tsv.gz'\n",
    "\n",
    "with open(gz_file_path, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "tsv_file_path = 'ip2asn-combined.tsv'\n",
    "with gzip.open(gz_file_path, 'rb') as f_in:\n",
    "    with open(tsv_file_path, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IPs: 100%|██████████| 20776/20776 [36:58<00:00,  9.37it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV saved to vpn_servers_all_providers_with_asn_and_description.csv\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Enhance the data to include ASN based on IP2ASN dataset using range-based integer lookups.\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import ipaddress\n",
    "import socket\n",
    "from tqdm import tqdm\n",
    "\n",
    "csv_file_path = 'vpn_servers_all_providers.csv'\n",
    "tsv_file_path = 'ip2asn-combined.tsv'\n",
    "\n",
    "# Function to convert an IP address to an integer to perform faster range based lookups\n",
    "def ip_to_int(ip):\n",
    "    return int(ipaddress.ip_address(ip))\n",
    "\n",
    "# Parse the IP2ASN TSV file into a list of tuples\n",
    "def load_tsv(tsv_path):\n",
    "    ip_ranges = []\n",
    "    with open(tsv_path, 'r') as file:\n",
    "        for line in file:\n",
    "            start, end, asn, _, description = line.strip().split('\\t', 4)\n",
    "            ip_ranges.append((ip_to_int(start), ip_to_int(end), asn, description))\n",
    "    return ip_ranges\n",
    "\n",
    "# Function to perform a rDNS lookup and resolve hostnames\n",
    "def resolve_hostname(hostname):\n",
    "    if not isinstance(hostname, str):  # Check if hostname is a string\n",
    "        return None  # If not, return None without attempting DNS lookup\n",
    "    try:\n",
    "        ip_list = socket.gethostbyname_ex(hostname)[2]\n",
    "        for ip in ip_list:\n",
    "            if ipaddress.ip_address(ip).version == 4:\n",
    "                return ip\n",
    "    except socket.gaierror:\n",
    "        return None\n",
    "\n",
    "# Function to find the ASN and its description for a given IP address\n",
    "def find_asn_and_description(ip_int, ip_ranges):\n",
    "    for start, end, asn, description in ip_ranges:\n",
    "        if start <= ip_int <= end:\n",
    "            return asn, description\n",
    "    return 'ASN not found', 'Description not found'\n",
    "\n",
    "# Initial load of the IP ranges from the TSV file\n",
    "ip_ranges = load_tsv(tsv_file_path)\n",
    "\n",
    "# Load the VPN endpoints CSV\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Add new columns for the ASN and ASN description - initialized to None\n",
    "df['ASN'] = None\n",
    "df['ASN_Description'] = None\n",
    "\n",
    "# Update the DataFrame with the ASN and its description for each IP\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing IPs\"):\n",
    "    ip_address = row['ips']\n",
    "    hostname = row['hostname'] if isinstance(row['hostname'], str) else None\n",
    "    if pd.isnull(ip_address) or ',' in ip_address or ip_address == '':  # Check for empty, NaN, or multiple IPs\n",
    "        ip_address = resolve_hostname(hostname)  # Attempt rDNS lookup with a valid hostname\n",
    "        if ip_address is None:\n",
    "            continue\n",
    "\n",
    "    try:\n",
    "        ip_int = ip_to_int(ip_address)\n",
    "        asn, description = find_asn_and_description(ip_int, ip_ranges)\n",
    "        df.at[index, 'ASN'] = asn\n",
    "        df.at[index, 'ASN_Description'] = description\n",
    "    except ValueError:\n",
    "        continue \n",
    "\n",
    "# Save the enhanced DataFrame to a new CSV file\n",
    "updated_csv_file_path = 'vpn_servers_all_providers_with_asn_and_description.csv'\n",
    "df.to_csv(updated_csv_file_path, index=False)\n",
    "\n",
    "print(f\"Updated CSV saved to {updated_csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarize the # of distinct VPN Providers using an ASN\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "file_path = 'vpn_servers_all_providers_with_asn_and_description.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "distinct_providers_per_asn = data.groupby('ASN')['provider'].nunique().reset_index()\n",
    "\n",
    "distinct_providers_per_asn.columns = ['ASN', 'Distinct_Providers']\n",
    "\n",
    "distinct_providers_per_asn['ASN'] = distinct_providers_per_asn['ASN'].astype(str)\n",
    "\n",
    "asn_descriptions = data[['ASN', 'ASN_Description']].drop_duplicates()\n",
    "asn_descriptions['ASN'] = asn_descriptions['ASN'].astype(str)\n",
    "\n",
    "distinct_providers_per_asn = distinct_providers_per_asn.merge(asn_descriptions, on='ASN', how='left')\n",
    "\n",
    "distinct_providers_per_asn.to_csv('distinct_providers_per_asn_with_description.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
